{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL2ZGbCWJA3T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# --- Step 1: Load and Analyze Data ---\n",
        "file_name = '/content/preprocessed_diabetes_data.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "print(f\"Successfully loaded '{file_name}'\")\n",
        "\n",
        "# Debug: Print column names and data types\n",
        "print(\"\\n--- Debug: Dataset Information ---\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Data types:\\n{df.dtypes}\")\n",
        "\n",
        "# --- Data Diagnosis ---\n",
        "print(\"\\n--- Data Diagnosis ---\")\n",
        "target_counts = df['Diabetes_Diagnosis'].value_counts()\n",
        "print(\"Distribution of Target Variable ('Diabetes_Diagnosis'):\")\n",
        "print(target_counts)\n",
        "print(\"----------------------\\n\")\n",
        "\n",
        "# --- Step 2: Prepare Data ---\n",
        "y = df['Diabetes_Diagnosis'].astype(int)  # Ensure target is integer\n",
        "X = df.drop('Diabetes_Diagnosis', axis=1)\n",
        "\n",
        "# Debug: Print feature information\n",
        "print(f\"Features: {list(X.columns)}\")\n",
        "print(f\"Feature data types:\\n{X.dtypes}\")\n",
        "\n",
        "# Create train-validation-test split (proper way)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# --- Step 3: Scale Data ---\n",
        "scaler = StandardScaler()\n",
        "columns_to_scale = ['Age', 'BMI']\n",
        "\n",
        "# Fit scaler only on training data\n",
        "X_train_scaled = X_train.copy()\n",
        "X_val_scaled = X_val.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
        "X_val_scaled[columns_to_scale] = scaler.transform(X_val[columns_to_scale])\n",
        "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
        "\n",
        "# --- Step 4: Calculate Class Weights ---\n",
        "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: weights[i] for i in range(len(weights))}\n",
        "print(f\"Calculated Class Weights: {class_weights}\")\n",
        "\n",
        "# --- Step 5: Build and Compile Model ---\n",
        "model = Sequential([\n",
        "    InputLayer(input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "custom_optimizer = Adam(learning_rate=0.0005)\n",
        "model.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# --- Step 6: Train the Model ---\n",
        "print(\"\\nStarting model training...\")\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1)\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled.values.astype(np.float32), y_train.values.astype(np.float32),\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_scaled.values.astype(np.float32), y_val.values.astype(np.float32)),  # Use proper validation set\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Step 7: Evaluate the Final Model ---\n",
        "loss, accuracy = model.evaluate(X_test_scaled.values.astype(np.float32), y_test.values.astype(np.float32), verbose=0)\n",
        "print(f\"\\nFinal Model Evaluation on Test Set:\")\n",
        "print(f\"  - Test Loss: {loss:.4f}\")\n",
        "print(f\"  - Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# --- Step 8: Save Model, Scaler, Feature Names, and History ---\n",
        "print(\"\\nSaving all artifacts...\")\n",
        "\n",
        "# Save model\n",
        "model.save('diabetes_model.keras')\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "\n",
        "# Save feature names for consistency (IMPORTANT!)\n",
        "feature_names = list(X.columns)\n",
        "joblib.dump(feature_names, 'feature_names.joblib')\n",
        "\n",
        "# Save columns that were scaled\n",
        "joblib.dump(columns_to_scale, 'scaled_columns.joblib')\n",
        "\n",
        "# Convert the history object to a DataFrame and save it as a CSV\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv('training_history.csv', index=False)\n",
        "\n",
        "print(\"Model, scaler, feature names, and training history saved successfully.\")\n",
        "print(f\"Feature order: {feature_names}\")\n",
        "print(\"\\nTraining script finished.\")"
      ]
    }
  ]
}